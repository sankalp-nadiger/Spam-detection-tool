{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea70a7e-5648-43ea-ba46-fa145825614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python313\\lib\\site-packages (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55200bf6-b8d8-4d03-8acf-febfa0b27e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python313\\lib\\site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.5 MB 6.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.5 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc867c1-c233-4525-902b-99e9e2efecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cffef4a8-f9d8-42fc-b02d-d36658925687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\ADMIN\\Downloads\\spam.csv',encoding='latin-1')\n",
    "df=df[['v1','v2']]\n",
    "df.columns=['category','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4b3e85-0401-42c8-966f-73867a8595fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].map({'ham': 0, 'spam': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a4c34f-1c61-4b25-b580-3d0136db60e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebc4cce3-fa98-404c-893b-0af217397e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = text.lower()             # Convert to lowercase\n",
    "    text = text.split()             # Tokenize\n",
    "    text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9054eaa-4f99-4bea-9bfa-8267db340baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000)  # Use the top 3000 features\n",
    "X = tfidf.fit_transform(df['processed_text']).toarray()\n",
    "y = df['category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867db803-9282-4059-aa91-5ec2074aa6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\python313\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.1 MB 9.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 19.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp313-cp313-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 7.1/44.5 MB 34.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 16.8/44.5 MB 35.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.2/44.5 MB 37.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.4/44.5 MB 38.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 40.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 40.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0eaafa7-2583-4932-bf62-723ea51ec82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb101374-f3ca-4278-af60-9077c45a686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9605381165919282\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       959\n",
      "           1       0.97      0.74      0.84       156\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.97      0.87      0.91      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6de6e69f-82de-4328-895b-25ecaaab6049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n"
     ]
    }
   ],
   "source": [
    "pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06259c2c-a9a9-4b59-bd26-66b379686871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('spam_detector.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e1e73ec-e2bc-4316-838a-4067b1d8535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text:  This is Mr. Ramareddy from CBI! It's observed that your credit card has been used in illegal operations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Spam\n"
     ]
    }
   ],
   "source": [
    "with open('spam_detector.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_tfidf = pickle.load(f)\n",
    "\n",
    "sample_text = input(\"Enter text: \");\n",
    "sample_text = preprocess_text(sample_text)\n",
    "sample_vector = loaded_tfidf.transform([sample_text]).toarray()\n",
    "prediction = loaded_model.predict(sample_vector)\n",
    "print(\"Spam\" if prediction[0] == 1 else \"Not Spam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8151c-1a13-477d-8841-bb7ceb093349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
