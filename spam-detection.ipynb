{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ea70a7e-5648-43ea-ba46-fa145825614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python313\\lib\\site-packages (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55200bf6-b8d8-4d03-8acf-febfa0b27e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python313\\lib\\site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1ae8d9-8b86-4eb7-8699-a5e201a91c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc867c1-c233-4525-902b-99e9e2efecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cffef4a8-f9d8-42fc-b02d-d36658925687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\ADMIN\\Downloads\\spam.csv',encoding='latin-1')\n",
    "df=df[['v1','v2']]\n",
    "df.columns=['category','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4b3e85-0401-42c8-966f-73867a8595fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].map({'ham': 0, 'spam': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a4c34f-1c61-4b25-b580-3d0136db60e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc4cce3-fa98-404c-893b-0af217397e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = text.lower()             # Convert to lowercase\n",
    "    text = text.split()             # Tokenize\n",
    "    text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9054eaa-4f99-4bea-9bfa-8267db340baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Features Shape: (5571, 4)\n",
      "Combined Features Shape: (5571, 54)\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(r\"D:\\glove.6B.50d.txt\")  # Path to GloVe file\n",
    "\n",
    "# Step 3: Compute Text Embeddings\n",
    "def get_average_embedding(text, embeddings, dimension=50):\n",
    "    tokens = text.split()\n",
    "    valid_vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if valid_vectors:\n",
    "        return np.mean(valid_vectors, axis=0)  # Average of word vectors\n",
    "    else:\n",
    "        return np.zeros(dimension)\n",
    "\n",
    "embedding_features = np.array([get_average_embedding(t, glove_embeddings) for t in df['processed_text']])\n",
    "\n",
    "def extract_additional_features(text):\n",
    "    # Feature: Length of the message\n",
    "    length = len(text)\n",
    "    # Feature: Count of uppercase words\n",
    "    uppercase_count = sum(1 for word in text.split() if word.isupper())\n",
    "    # Feature: Presence of monetary terms\n",
    "    has_money = int(bool(re.search(r'\\$\\d+', text)))\n",
    "    has_urgent = int('urgent' in text.lower())\n",
    "    return [length, uppercase_count, has_money, has_urgent]\n",
    "\n",
    "additional_features = np.array([extract_additional_features(t) for t in df['text']])\n",
    "\n",
    "# Combine embeddings and Additional Features\n",
    "X_combined = np.hstack([embedding_features, additional_features])\n",
    "\n",
    "#Feature shapes\n",
    "print(\"Additional Features Shape:\", additional_features.shape)\n",
    "print(\"Combined Features Shape:\", X_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a135329-d9a4-42f8-81d5-e843bb18d1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867db803-9282-4059-aa91-5ec2074aa6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python313\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\python313\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0eaafa7-2583-4932-bf62-723ea51ec82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y=df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb101374-f3ca-4278-af60-9077c45a686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9372197309417041\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       959\n",
      "           1       0.79      0.75      0.77       156\n",
      "\n",
      "    accuracy                           0.94      1115\n",
      "   macro avg       0.88      0.86      0.87      1115\n",
      "weighted avg       0.94      0.94      0.94      1115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06259c2c-a9a9-4b59-bd26-66b379686871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a message to classify as Spam or Not Spam:   Thank you for paying last month’s bill. We’re rewarding our very best customers with a gift for their loyalty. Click here! [Link]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The message is classified as: Spam\n"
     ]
    }
   ],
   "source": [
    "def predict_message_spam_status(message, model, embeddings, stop_words, stemmer, dimension=50):\n",
    "    \"\"\"\n",
    "    Predict if a given message is spam or not.\n",
    "    \n",
    "    Parameters:\n",
    "        message (str): The input message to classify.\n",
    "        model (object): The trained Logistic Regression model.\n",
    "        embeddings (dict): The GloVe word embeddings.\n",
    "        stop_words (set): Set of stopwords to exclude during preprocessing.\n",
    "        stemmer (PorterStemmer): Stemmer for preprocessing.\n",
    "        dimension (int): Dimensionality of the GloVe vectors.\n",
    "    \n",
    "    Returns:\n",
    "        str: \"Spam\" or \"Not Spam\"\n",
    "    \"\"\"\n",
    "    # Preprocess the message\n",
    "    processed_message = preprocess_text(message)\n",
    "    \n",
    "    # Compute the average embedding\n",
    "    embedding_vector = get_average_embedding(processed_message, embeddings, dimension)\n",
    "    \n",
    "    # Extract additional features\n",
    "    additional_features_vector = extract_additional_features(message)\n",
    "    \n",
    "    # Combine embeddings and additional features\n",
    "    combined_features = np.hstack([embedding_vector, additional_features_vector])\n",
    "    \n",
    "    # Predict using the model\n",
    "    prediction = model.predict([combined_features])[0]\n",
    "    return \"Spam\" if prediction == 1 else \"Not Spam\"\n",
    "\n",
    "# Example usage\n",
    "input_message = input(\"Enter a message to classify as Spam or Not Spam: \")\n",
    "result = predict_message_spam_status(input_message, model, glove_embeddings, stop_words, stemmer)\n",
    "print(f\"The message is classified as: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8151c-1a13-477d-8841-bb7ceb093349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
